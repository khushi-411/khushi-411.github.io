---
layout: post
title: Introduction to Parallel Programming and CPU-GPU Architectures
date: 2023-08-11
category: CUDA
tags:
- cpp
redirect_from:
- /cpp/2023/08/11/gpu_intro/
- /gpu_intro.html
---

Hi there! I've been diving into the world of parallel programming through
the excellent book "Programming Massively Parallel Processors:
A Hands-on Approach" by [Wen-mei W. Hwu](https://scholar.google.com/citations?user=ohjQPx8AAAAJ&hl=en),
[David B. Kirk](https://scholar.google.com/citations?user=fMbArPwAAAAJ&hl=en),
and [Izzat El Hajj](https://scholar.google.com/citations?user=_VVw504AAAAJ&hl=en).
The content of this blog post is motivated by the book's first chapter:
Introduction, the second chapter: Heterogeneous data-parallel Computing,
and the Intel blogs on vectorization.

I often see folks working on the speed of execution of programs in 
GPUs and CPUs and reach the conclusion GPUs outperform in many cases.
Well, there's a lot of interesting stuff under the hood.

The evolution of these microprocessors started in the early 1980s.
From a single-core processor to a double-core processor, then to a
multicore processor (typically just increasing the number of physical CPUs,
but the operations were performed in sequential steps).
It brought significant changes, but it does not actually increases the speed
of execution; it just adds another processor core. We know performance is a
never-ending process, and we need technology to produce realistic effects
similar to what we saw in Marvel movies. NVIDIA introduced a new generation
of microprocessors back in the 1990s in which we can run programs parallelly,
aka multiple processes can be performed simultaneously. Since then,
there have been significant developments in processor speed.
Let's dig in more about parallel computing.

### Heterogeneous parallel computing
Following up on the earlier introduction. We saw how the processor
evaluated by the time. And the two different designs were introduced,
one by Intel & AMD and the other by NVIDIA.
1. **Multicore Trajectory**: The team increased the number of processor/ CPU
cores generation after generation to increase the performance.
Since they have multiple cores, they can process multiple instructions
simultaneously, but the new execution of the subsequent instructions
starts after the competition of one instruction. The most recent Intel
processor has a 24-core, which could run 0.33 TFLOPS for FP64 and 0.66 TFLOPS for FP32.
2. **Many Threaded Trajectory**: In this trajectory, all the programs are
processed in parallel; even if the previous instruction hasn't been completed,
thousands of instructions can run parallelly. The support of the number of
threads to run parallelly was increased generation after generation.
The most recent NVIDIA arch can run 9.7 TFLOPS for FP64, 156 TFPOS for FP32,
and 312 TFLOPS for FP16 datatypes, which is incredible results compared
to multicore trajectories.

#### Why is there such a large peak performance difference between multicore CPU and many-thread GPU?
The answers lie within the core design structure the companies
selected to follow: the CPUs are designed to optimize the
sequential code, while the GPUs are designed to perform a massive number
of operations simultaneously. Please see the pictures below, which depict
the fundamental architectural difference between the CPU and the GPU.
- The amount of space for an arithmetic logic unit for the CPU is
comparatively very small compared to GPU. The reason lies that we
could only perform sequential operations at a time in CPU, so
Intel dedicated a small amount of space for performing these operations,
while in GPU, we need more space to perform as many operations as possible,
so NVIDIA dedicated more space to it; the most recent GPU support 1024
threads per block (we'll get into more details about this in later blog posts).
- On the other hand, we need to store more data in CPU memory because
more data will be waiting for their turn, so Intel dedicated more Cache
memory in their processor. In comparison, NVIDIA GPUs don't really require
more space to keep the data, so they dedicated less space for Cache memory.
It is still challenging for NVIDIA GPUs because the speed depends on how data
can be delivered from the memory system to the processor and vice versa.
We will read more about this topic in my upcoming blog posts.
- Coming back to the point, Intel's goal is to reduce the latency of an
operation so that many operations can be completed in a short time interval,
while NVIDIA's goal is to increase the number of operations performed at a
single instant, aka throughput oriented.

<p float="left">
    <img src="/assets/CUDA/cpu_arch.png" width="200" />
    <img src="/assets/CUDA/gpu_arch.png" width="200" />
</p>

With time it was observed that GPUs perform great in cases when we have
to perform a massive number of operations at the same time. But still,
under the hood, GPUs cannot completely replace the use of CPUs; we still
need to use CPU memory for a smaller number of operations.

This led to the introduction of **CUDA (Compute Unified Device Architecture)**.
NVIDIA designed it to support the joint execution of CPU-GPU applications
and for General Purpose Parallel Programming (GPGPU). GPGPUs are used to
perform general mathematical computations on GPUs. NVIDIA added additional
hardware to the chip to run these programs. And the speed of execution
depends on the amount of data that can be parallelized. But keep in note
the most crucial factor in speeding up is how fast you can read and write
the data from memory. Various fusion methods and parallelization techniques
are developed to decrease the gaps between the hardware and software
integration which we'll read later.
Let's hop into some important considerations we need to have if we want to write a CUDA program:
1. identify the part of application programs to be parallelized
2. isolating the data to be used by the parallelized code
3. using an API function to transfer data to the parallel computing device
4. developing parallel part into a kernel function for execution by parallel threads
5. launching a kernel function for the execution of parallel threads
6. eventually, transferring the data back to the host processor with an API function call

### Let's try some CUDA and Vectorized Code Examples
Here's an example of vector addition in CUDA GPUs. We made a call via a
host memory (`vecAdd`) to a global function (a function that can be
written and read from both host and device); here, a kernel function,
`vecAddKernel`, specifies that all threads should execute it, each on the
different part of data. In GPUs, a single program split across multiple data, aka SPMD.
```cu
#include <iostream>

// access by both __host__ & __device__ functions
__global__ void vecAddKernel(float* A, float* B, float* C, int n) {
    // local variable, generated for each thread
    // each thread follows the SIMD process
    int i = threadIdx.x + blockDim.x * blockIdx.x;
    if (i < n) {
        C[i] = A[i] + B[i];
    }
}

// host memory
void vecAdd(float* A_h, float* B_h, float* C_h, int n) {
    // size in bytes
    int size = n * sizeof(float);
    float *A_d, *B_d, *C_d;

    // allocating memory in GPU, the function accepts a generic pointer
    // returns generic objects
    cudaMalloc((void**)&A_d, size);
    cudaMalloc((void**)&B_d, size);
    cudaMalloc((void**)&C_d, size);

    // to transfer data from host to device
    cudaMemcpy(A_d, A_h, size, cudaMemcpyHostToDevice);
    cudaMemcpy(B_d, B_h, size, cudaMemcpyHostToDevice);

    dim3 dimGrid(ceil(n / 256.0), 1, 1);  // number of blocks in a grid
    dim3 dimBlock(256, 1, 1);  // number of threads in each block
    vecAddKernel<<<dimGrid, dimBlock>>>(A_d, B_d, C_d, n);

    cudaMemcpy(C_h, C_d, size, cudaMemcpyDeviceToHost);

    // to free storage space of global memory
    cudaFree(A_d);
    cudaFree(B_d);
    cudaFree(C_d);
}

int main() {
    float A_h[] = {1.0, 2.0, 3.0, 4.0};
    float B_h[] = {4.0, 5.0, 6.0, 7.0};
    float C_h[3];

    vecAdd(A_h, B_h, C_h, 3);

    for (int i = 0; i < 3; i++) {
        std::cout << C_h[i] << " ";
    }
    std::cout << std::endl;

    return 0;
}
```

Let's see a vector addition example in Intel CPUs using vectorization.
Vectorization is a way to write parallel operations in CPUs.
In modern CPUs, these are implemented via Advanced Vector Extensions (AVX);
some of their types are AVX1, AVX2, AVX512, etc. We'll use AVX128 in our example.
In CPUs, each instruction processes multiple data, aka SIMD, these use
vectorization techniques, and AVX are modern ways to implement vectorization
in CPUs. Note that each intrinsic operation is placed in a
registerâ€”more info on [Intel's blog](https://www.intel.com/content/www/us/en/docs/cpp-compiler/developer-guide-reference/2021-8/arithmetic-intrinsics-001.html).
```cpp
#include <emmintrin.h>  // to include Intel's SSE2 intrinsics
#include <iostream>

int main() {
    // _m128: it is a data type with SIMD extensions
    // defined on 16-byte boundaries, similar to int, long dtypes
    __m128 v1 = _mm_set_ps(1.0, 2.0, 3.0, 4.0);
    __m128 v2 = _mm_set_ps(4.0, 5.0, 6.0, 7.0);

    __m128 r = _mm_add_ps(v1, v2);

    float res[8];
    _mm_store_ps(res, r);

    for (int i = 3; i >= 0; i--) {
        std::cout << res[i] << " ";
    }
    std::cout << std::endl;

    return 0;
}
```

### Conclusion
So, had fun? This blog post was just a gentle introduction to
parallel programming using CUDA. We started the blog posts with the
evolution of CPUs, which led to the introduction of GPUs. We then
read about how their architectural designs cause such a huge
difference in performance. Later, we ended the blog posts by playing
with some cool examples to write a vector addition kernel in CUDA and
CPU via vectorization.

Hope you enjoyed it! I would love to know your thoughts and anything
interesting you learned in your journey toward understanding parallel
programming. Feel free to drop a  comment below. See you next time via
my next exciting blog post for writing an optimized Matrix
Multiplication kernel! Stay tuned!
